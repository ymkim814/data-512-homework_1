{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Article Page Views API Example\n",
    "This example illustrates how to access page view data using the [Wikimedia REST API](https://www.mediawiki.org/wiki/Wikimedia_REST_API). This example shows how to request monthly counts of page views for one specific article. The API documentation, [pageviews/per-article](https://wikimedia.org/api/rest_v1/#/Pageviews%20data), covers additional details that may be helpful when trying to use or understand this example.\n",
    "\n",
    "## License\n",
    "This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.3 - August 16, 2024\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This block imports necessary packages for API calling, parsing, analysis, and visualization of the API page view data. All packages can be installed by conda activate 'package name' command in anaconda or pip install 'package name' in python/notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To-Do: Link addition, file name change & upload, create new virtual envir, requirement txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are standard python modules\n",
    "import json, time, urllib.parse\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read the csv file that contains the subset of article we want to call from API. The format of this file is csv and we mainly use 'disease' column later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_subset = pd.read_csv('rare-disease_cleaned.AUG.2024.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example relies on some constants that help make the code a bit more readable.\n",
    "We mainly call the pageviews API using the article name, while controlling wih constraints (not exceeding 100 requests per second). In this code, we use parameteres of project,access (3 types), agent, article (title of disease), granularity (time measurement), time frame (start and end date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The REST API 'pageviews' URL - this is the common URL/endpoint for all 'pageviews' API requests\n",
    "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
    "\n",
    "# This is a parameterized string that specifies what kind of pageviews request we are going to make\n",
    "# In this case it will be a 'per-article' based request. The string is a format string so that we can\n",
    "# replace each parameter with an appropriate value before making the request\n",
    "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
    "\n",
    "# The Pageviews API asks that we not exceed 100 requests per second, we add a small delay to each request\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making a request to the Wikimedia API they ask that you include your email address which will allow them\n",
    "# to contact you if something happens - such as - your code exceeding rate limits - or some other error \n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<ymkim814@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = article_subset['disease']\n",
    "\n",
    "# This template is used to map parameter values into the API_REQUST_PER_ARTICLE_PARAMS portion of an API request. The dictionary has a\n",
    "# field/key for each of the required parameters. In the example, below, we only vary the article name, so the majority of the fields\n",
    "# can stay constant for each request. Of course, these values *could* be changed if necessary.\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"\",      # this should be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015070100\",   # start and end dates need to be set\n",
    "    \"end\":         \"2024093000\"    # this is likely the wrong end date\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article pages. Therefore the parameter most likely to change is the article_title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageviews_per_article(article_title = None, \n",
    "                                  access_type = None,\n",
    "                                  endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                  endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                  request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                  headers = REQUEST_HEADERS):\n",
    "\n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['article'] = article_title\n",
    "    \n",
    "    if access_type:\n",
    "        request_template['access'] = access_type\n",
    "        \n",
    "\n",
    "    if not request_template['article']:\n",
    "        raise Exception(\"Must supply an article title to make a pageviews request.\")\n",
    "\n",
    "    # Titles are supposed to have spaces replaced with \"_\" and be URL encoded\n",
    "    article_title_encoded = urllib.parse.quote(request_template['article'].replace(' ','_'))\n",
    "    request_template['article'] = article_title_encoded\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This for loop takes article name and access as parameters and call the API for pageviews. For example, we can get 'desktop' access or 'mobile-app' and add 'mobile-web' view in the code. The result is a dictionary for each article (disease) like this:\\\n",
    "    \"Klinefelter syndrome\": [ \\\n",
    "        {\\\n",
    "            \"project\": \"en.wikipedia\",\\\n",
    "            \"article\": \"Klinefelter_syndrome\",\\\n",
    "            \"granularity\": \"monthly\",\\\n",
    "            \"timestamp\": \"2015070100\",\\\n",
    "            \"agent\": \"user\",\\\n",
    "            \"views\": 36798\\\n",
    "        }, ...\n",
    "\n",
    "where each article includes whole timestamp if exists.\n",
    "\n",
    "The outer for loop runs for each article while the inner article runs for each timestamp adding. At the end, we save the output dictionary as a form of .json file into the same folder we have the notebook file. One can add path lik ,'w', path if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_desktop_views = {}\n",
    "for i in ARTICLE_TITLES:\n",
    "    print(\"Getting pageview data for:\", i, \"type\", 'desktop')\n",
    "    desk_views = request_pageviews_per_article(i, 'desktop')    \n",
    "    desktop_views = []\n",
    "\n",
    "    if 'items' in desk_views:\n",
    "        for info in desk_views['items']:\n",
    "            view_entry = {\n",
    "                'project': info['project'],\n",
    "                'article': info['article'],\n",
    "                'granularity': info['granularity'],\n",
    "                'timestamp': info['timestamp'],\n",
    "                'agent': info['agent'],\n",
    "                'views': info['views']  \n",
    "            }\n",
    "\n",
    "            desktop_views.append(view_entry)\n",
    "\n",
    "    # Store the list of combined views for the current article in all_mobile_views_test\n",
    "    all_desktop_views[i] = desktop_views\n",
    "\n",
    "   \n",
    "with open('desktop_views.json', 'w') as outfile:\n",
    "    json.dump(all_desktop_views, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below has he same logic as above but just add web-mobile to app-mobile to have combined total mobile view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mobile_views = {}\n",
    "for i in ARTICLE_TITLES:\n",
    "    print(\"Getting pageview data for:\", i, \"type\", 'mobile-app')\n",
    "    app_views = request_pageviews_per_article(i, 'mobile-app')\n",
    "    print(\"Getting pageview data for:\", i, \"type\", 'mobile-web')\n",
    "    web_views = request_pageviews_per_article(i, 'mobile-web')    \n",
    " \n",
    "    # Initialize a list for the current article\n",
    "    mobile_views = []\n",
    "\n",
    "    # Check if app_views has 'items'\n",
    "    if 'items' in web_views:\n",
    "        for info in web_views['items']:\n",
    "            view_entry = {\n",
    "                'project': info['project'],\n",
    "                'article': info['article'],\n",
    "                'granularity': info['granularity'],\n",
    "                'timestamp': info['timestamp'],\n",
    "                'agent': info['agent'],\n",
    "                'views': info['views']  \n",
    "            }\n",
    "            #print(info['views'])\n",
    "            if 'items' in app_views:\n",
    "                for app_info in app_views['items']:\n",
    "                    if app_info['timestamp'] == info['timestamp']: \n",
    "                        view_entry['views'] += app_info['views'] \n",
    "                        #print(web_info['views'])\n",
    "                        break  # No need to continue checking once we find the matching timestamp\n",
    "            \n",
    "=            mobile_views.append(view_entry)\n",
    "\n",
    "    all_mobile_views[i] = mobile_views\n",
    "\n",
    "    \n",
    "with open('combined_mobile_views.json', 'w') as outfile:\n",
    "    json.dump(all_mobile_views, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below has he same logic as above but just add web-mobile, app-mobile, and desktop view to have combined total view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_views = {}\n",
    "for i in ARTICLE_TITLES:\n",
    "    print(\"Getting pageview data for:\", i, \"type\", 'mobile-app')\n",
    "    app_views = request_pageviews_per_article(i, 'mobile-app')\n",
    "    print(\"Getting pageview data for:\", i, \"type\", 'mobile-web')\n",
    "    web_views = request_pageviews_per_article(i, 'mobile-web')    \n",
    "    print(\"Getting pageview data for:\", i, \"type\", 'desktop')\n",
    "    desk_views = request_pageviews_per_article(i, 'desktop')    \n",
    "\n",
    "    total_views_sum = []\n",
    "\n",
    "    # Check if app_views has 'items'\n",
    "    if 'items' in desk_views:\n",
    "        for info in desk_views['items']:\n",
    "            view_entry = {\n",
    "                'project': info['project'],\n",
    "                'article': info['article'],\n",
    "                'granularity': info['granularity'],\n",
    "                'timestamp': info['timestamp'],\n",
    "                'agent': info['agent'],\n",
    "                'views': info['views'] \n",
    "            }\n",
    "\n",
    "            if 'items' in app_views:\n",
    "                for app_info in app_views['items']:\n",
    "                    if app_info['timestamp'] == info['timestamp']:  \n",
    "                        view_entry['views'] += app_info['views'] \n",
    "\n",
    "            if 'items' in web_views:\n",
    "                for web_info in web_views['items']:\n",
    "                    if web_info['timestamp'] == info['timestamp']:  \n",
    "                        view_entry['views'] += web_info['views']  \n",
    "\n",
    "                        break  # No need to continue checking once we find the matching timestamp\n",
    "            \n",
    "            total_views_sum.append(view_entry)\n",
    "\n",
    "    total_views[i] = total_views_sum\n",
    "   \n",
    "with open('total_views.json', 'w') as outfile:\n",
    "    json.dump(total_views, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(json.dumps(views,indent=4))\n",
    "# print(f\"Collected {len(views['items'])} months of pageview data\")\n",
    "# for month in views['items']:\n",
    "#     print(json.dumps(month,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above output should show dictionaries with views per month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we finished exporting desktop file and combined mobile views, we exported them as json file since API calling takes long time - which is inefficient to call them frequntly. Then we normalize json file format to create as pandas dataframe format. The dataframe consists of seven columns - project, article, granularity, timestamp, agent, views, and month. Months is corresponding timestampe, but it was created for easier visualization of monthly time series and legend creation. We will mainly use month, article, and views for analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data from both desktop and mobile views\n",
    "with open('rare-disease_monthly_desktop_201507-202409.json', 'r') as desktop_file:\n",
    "    desktop_data = json.load(desktop_file)\n",
    "\n",
    "with open('rare-disease_monthly_mobile_201507-202409.json', 'r') as mobile_file:\n",
    "    mobile_data = json.load(mobile_file)\n",
    "\n",
    "# Function to process and normalize the data\n",
    "def process_data(json_data):\n",
    "    df_list = []\n",
    "    for disease, entries in json_data.items():\n",
    "        temp_df = pd.json_normalize(entries)\n",
    "        df_list.append(temp_df)\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H')\n",
    "    df['month'] = df['timestamp'].dt.to_period('M')\n",
    "    return df\n",
    "\n",
    "# Process desktop and mobile data\n",
    "desktop_df = process_data(desktop_data)\n",
    "mobile_df = process_data(mobile_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Maximum Average and Minimum Average\n",
    "\n",
    "This code calculates average pageview of each disease by group by each disease and get the average of views. It creates a new dataframe with article name and its average views. Columns are renamed to prevent any naming confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop_avg_views = desktop_df.groupby('article')['views'].mean().reset_index()\n",
    "mobile_avg_views = mobile_df.groupby('article')['views'].mean().reset_index()\n",
    "\n",
    "# Rename the average columns\n",
    "desktop_avg_views.columns = ['article', 'avg_views']\n",
    "mobile_avg_views.columns = ['article', 'avg_views']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code selects the highest and smallest average view for mobile and desktop views and create the table with views and article names. Then we filtered the original dataset for only those 4 articles to have timestamp and views for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the articles with the highest and lowest average views\n",
    "max_desktop = desktop_avg_views.nlargest(1, 'avg_views')\n",
    "min_desktop = desktop_avg_views.nsmallest(1, 'avg_views')\n",
    "max_mobile = mobile_avg_views.nlargest(1, 'avg_views')\n",
    "min_mobile = mobile_avg_views.nsmallest(1, 'avg_views')\n",
    "\n",
    "# Combine the selected articles into a single DataFrame\n",
    "selected_articles = pd.concat([max_desktop, min_desktop, max_mobile, min_mobile])\n",
    "\n",
    "max_desktop_data = desktop_df[desktop_df['article'] == max_desktop['article'].values[0]]\n",
    "min_desktop_data = desktop_df[desktop_df['article'] == min_desktop['article'].values[0]]\n",
    "max_mobile_data = mobile_df[mobile_df['article'] == max_mobile['article'].values[0]]\n",
    "min_mobile_data = mobile_df[mobile_df['article'] == min_mobile['article'].values[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we plot the result using matplotlib library in python. We plot 4 lines, while using different linestyle for mobile and desktop to distinguish them. The title reflects what we plot and month with page views. The ticker is rotated for visibility and savefig saves the visualization as .png file in the same folder as this notebook. We can see that for both desktop and mobile access, black death is the highest and filppi_syndrom is the lowest average page requests. The peak timestamp is also similar, which is early 2020. Howver, mobile views has much higher view for the peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plotting for Max Desktop Article\n",
    "plt.plot(max_desktop_data['month'].dt.to_timestamp(), max_desktop_data['views'], \n",
    "         label=f'Max Desktop: {max_desktop[\"article\"].values[0]}', linestyle='-', color='blue')\n",
    "\n",
    "# Plotting for Min Desktop Article\n",
    "plt.plot(min_desktop_data['month'].dt.to_timestamp(), min_desktop_data['views'], \n",
    "         label=f'Min Desktop: {min_desktop[\"article\"].values[0]}', linestyle='-', color='lightblue')\n",
    "\n",
    "# Plotting for Max Mobile Article\n",
    "plt.plot(max_mobile_data['month'].dt.to_timestamp(), max_mobile_data['views'], \n",
    "         label=f'Max Mobile: {max_mobile[\"article\"].values[0]}', linestyle='--', color='orange')\n",
    "\n",
    "# Plotting for Min Mobile Article\n",
    "plt.plot(min_mobile_data['month'].dt.to_timestamp(), min_mobile_data['views'], \n",
    "         label=f'Min Mobile: {min_mobile[\"article\"].values[0]}', linestyle='--', color='lightcoral')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Articles with Highest and Lowest Average Page Requests (Desktop and Mobile)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Page Views')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize='small')\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Maximum Average and Minimum Average.png')  # Save the plot as a PNG file\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 10 Peak Page Views\n",
    "\n",
    "In this code block, we get the value for the max views for each article and make them as dataframe - with article, views, and months. We rename views for desktop and mobile since we need to merge them on article for comprehensive visualization visualization and distinguish them. The month is automatically renamed as month_x for desktop and month_y for mobile. Join is outer to include all information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find peak views for each article\n",
    "def find_peak_views(df):\n",
    "    peak_views = df.loc[df.groupby('article')['views'].idxmax()]  \n",
    "    return peak_views[['article', 'views', 'month']]\n",
    "\n",
    "desktop_peak_views = find_peak_views(desktop_df).rename(columns={'views': 'peak_views_desktop'})\n",
    "mobile_peak_views = find_peak_views(mobile_df).rename(columns={'views': 'peak_views_mobile'})\n",
    "\n",
    "# Merge the peak views data on article names\n",
    "combined_peak_views = pd.merge(desktop_peak_views, mobile_peak_views, on='article', how='outer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we selected the top 10 article for desktop views and another top 10 for mobile views. One can adjust the parameter to change it from 10 to 5 or 15 with nlargest function. Then like min and max average, we made them as dataframe to filter original data to have only top 10 views for each access type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by peak views and select the top 10 for desktop and mobile\n",
    "top_desktop = combined_peak_views.nlargest(10, 'peak_views_desktop')\n",
    "top_mobile = combined_peak_views.nlargest(10, 'peak_views_mobile')\n",
    "\n",
    "# Combine top articles\n",
    "top_articles = pd.concat([top_desktop[['article', 'peak_views_desktop']], top_mobile[['article', 'peak_views_mobile']]])\n",
    "\n",
    "# Filter the original DataFrames for only the top articles\n",
    "top_desktop_articles = desktop_df[desktop_df['article'].isin(top_articles['article'])]\n",
    "top_mobile_articles = mobile_df[mobile_df['article'].isin(top_articles['article'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a plot that has total 20 timeseries, top 10 for desktop and top 10 for mobile. We created access - article legend for clarity while making them different styles. We also rotated and adjustd size of label for visibility. savefig exports this visualization to the same folder with notebook as .png file. We can see that the Pandemic and Black Death has very high peak page views, especially in 2020. Regarding max views, mobile and desktop top 10 do not exactly match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for article in top_desktop['article']:\n",
    "    article_data = top_desktop_articles[top_desktop_articles['article'] == article]\n",
    "    plt.plot(article_data['month'].dt.to_timestamp(), article_data['views'], label=f'Desktop - {article}')\n",
    "\n",
    "for article in top_mobile['article']:\n",
    "    article_data = top_mobile_articles[top_mobile_articles['article'] == article]\n",
    "    plt.plot(article_data['month'].dt.to_timestamp(), article_data['views'], label=f'Mobile - {article}', linestyle='--')\n",
    "\n",
    "plt.title('Top 10 Articles by Peak Page Views (Desktop and Mobile)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Page Views')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize='small')\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Top 10 Peak Page Views.png')  # Save the plot as a PNG file\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fewest Months of Data\n",
    "In this code block, we count the number of unique months for each article since we already have month column. Then we rename the column to prevent any confusion. At this time, we use nsmallest with the number of 10 to select articles with shortest months of data. Then we used that dataframe to have timestamp of those 10 fewest month articles for each access type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique months for each article for desktop and mobile\n",
    "desktop_months_count = desktop_df.groupby('article')['month'].nunique().reset_index()\n",
    "mobile_months_count = mobile_df.groupby('article')['month'].nunique().reset_index()\n",
    "\n",
    "desktop_months_count.columns = ['article', 'months_available']\n",
    "mobile_months_count.columns = ['article', 'months_available']\n",
    "\n",
    "desktop_fewest_months = desktop_months_count.nsmallest(10, 'months_available')\n",
    "mobile_fewest_months = mobile_months_count.nsmallest(10, 'months_available')\n",
    "\n",
    "top_desktop_articles = desktop_df[desktop_df['article'].isin(desktop_fewest_months['article'])]\n",
    "top_mobile_articles = mobile_df[mobile_df['article'].isin(mobile_fewest_months['article'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we used matplotlib to plot them - although majority of them have very low page views, short period of data does not always indicate small number of page views, especialyl Covid 19 relevant ones. We created access - article legend for clarity while making them different styles. We also rotated and adjustd size of label for visibility. savefig exports this visualization to the same folder with notebook as .png file. In this case, top 10 for mobile and desktop are the same because it is not about the view but timeframe length. Hence, I made the color same for the same article, making it easier to compare the views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m desktop_fewest_months[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      4\u001b[0m     article_data \u001b[38;5;241m=\u001b[39m top_desktop_articles[top_desktop_articles[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m article]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for article in desktop_fewest_months['article']:\n",
    "    article_data = top_desktop_articles[top_desktop_articles['article'] == article]\n",
    "    plt.plot(article_data['month'].dt.to_timestamp(), article_data['views'], label=f'Desktop - {article}', linestyle='-')\n",
    "\n",
    "for article in desktop_fewest_months['article']:\n",
    "    article_data = top_mobile_articles[top_mobile_articles['article'] == article]\n",
    "    plt.plot(article_data['month'].dt.to_timestamp(), article_data['views'], label=f'Mobile - {article}', linestyle='--')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Top 10 Articles with Fewest Months of Data (Desktop and Mobile)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Page Views')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize='small')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Fewest Months of Data.png')  \n",
    "plt.show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
